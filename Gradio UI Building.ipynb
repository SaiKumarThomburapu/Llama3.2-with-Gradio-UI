{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36b43d8-bf82-4074-8e27-679e4ca2bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f994733b-196b-414f-8281-990aae8e9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bc0cec7-2377-44d9-98c7-59f6fd62b6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.27.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.9.0 (from gradio)\n",
      "  Downloading gradio_client-1.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\mastan\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (2.8.2)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.7-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from gradio-client==1.9.0->gradio) (2024.6.1)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.9.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mastan\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mastan\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mastan\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mastan\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Downloading gradio-5.27.0-py3-none-any.whl (54.0 MB)\n",
      "   ---------------------------------------- 0.0/54.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/54.0 MB 7.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.9/54.0 MB 7.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 4.2/54.0 MB 7.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 5.8/54.0 MB 6.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 6.8/54.0 MB 6.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 8.4/54.0 MB 6.5 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 10.2/54.0 MB 6.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 11.8/54.0 MB 7.0 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 13.4/54.0 MB 6.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 14.7/54.0 MB 6.9 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 15.7/54.0 MB 6.7 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 17.0/54.0 MB 6.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 18.6/54.0 MB 6.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 19.9/54.0 MB 6.6 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 20.7/54.0 MB 6.5 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 21.0/54.0 MB 6.1 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 21.8/54.0 MB 6.0 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 22.5/54.0 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 23.3/54.0 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 24.4/54.0 MB 5.7 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 25.4/54.0 MB 5.7 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 26.5/54.0 MB 5.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 27.3/54.0 MB 5.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 28.6/54.0 MB 5.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 29.6/54.0 MB 5.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 30.9/54.0 MB 5.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 32.2/54.0 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 33.6/54.0 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 33.6/54.0 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 33.8/54.0 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 34.1/54.0 MB 5.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 35.4/54.0 MB 5.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 37.0/54.0 MB 5.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 38.3/54.0 MB 5.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 39.6/54.0 MB 5.3 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 40.9/54.0 MB 5.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 42.2/54.0 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 43.8/54.0 MB 5.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 45.4/54.0 MB 5.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 46.9/54.0 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 48.8/54.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 50.1/54.0 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 51.4/54.0 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 51.9/54.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  52.7/54.0 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  53.7/54.0 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 54.0/54.0 MB 5.4 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.9.0-py3-none-any.whl (322 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading orjson-3.10.16-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.7-py3-none-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 6.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.4/11.6 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.5/11.6 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 6.5 MB/s eta 0:00:00\n",
      "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Using cached ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Installing collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, groovy, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, typer, safehttpx, gradio-client, fastapi, gradio\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.11.1\n",
      "    Uninstalling tomlkit-0.11.1:\n",
      "      Successfully uninstalled tomlkit-0.11.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.27.0 gradio-client-1.9.0 groovy-0.1.2 huggingface-hub-0.30.2 orjson-3.10.16 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 typer-0.15.2 uvicorn-0.34.2 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.5.1 requires ipython!=8.17.1,<9.0.0,>=8.13.0; python_version > \"3.8\", but you have ipython 9.0.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db5ba24a-305f-4466-a35c-8dccd0e89f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"You are an AI Assistant, please provide the answers as a professional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ddedc43-f797-4e24-8b75-ac2240558afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c41d037-0476-4aae-99d5-975a94b0dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap a call to llama3.2 in a simple function\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='llama3.2',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b3cbf1b-64ed-4b92-9035-daa1b5c9e37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm a large language model, I don't have real-time access to the current date. However, I can tell you that my knowledge cutoff is December 2023, so if you want to know what day of the week or month it was on a specific date after that, I'd be happy to help with that information.\\n\\nIf you provide me with a specific year and/or month, I can try to determine the correct answer. Alternatively, I can suggest ways for you to find out what the current day of the week is.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt(\"What is the day of this date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9e4a9d8-c46e-46fb-9b02-9771de08dd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'December 11th, 2001 falls on a Tuesday in different parts of the world. However, considering the International Date Line (IDL), December 11th, 2001, would be:\\n\\n- A Monday in UTC+0 time zone (Greenwich Mean Time)\\n- A Tuesday in UTC-5 time zone (Eastern Standard Time, United States)\\n- A Wednesday on the West Coast of North America\\n\\nAs for events that occurred on this date:\\n\\n* The US Federal Bureau of Investigation (FBI) issued a memo describing the FBI\\'s initial understanding of the 9/11 plot, known as the \"Pentagon Strike Plot\".'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt(\"what is the day of 11th december 2001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bfb8b8b4-8898-465a-a92a-aafcb49b3ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(name):\n",
    "    return name.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79986746-1b43-408d-a4bc-09735fd36d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BABBLU'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout(\"Babblu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28ac3725-7cc4-44ad-8ef6-6c383d270253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inputs and Outputs\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=shout,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87080f5f-12e4-4e41-984f-28b1cc335961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28dad6d0-6d62-45ba-9480-6c1b84125020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt=\"You are an AI Assistant i need you to give all the response using markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36fd81e5-7033-4a75-a6a8-ad3b4f78b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='llama3.2',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f713c314-fe33-474b-a13b-5c83991a31f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Using cached google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.168.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from google-generativeai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from google-generativeai) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mastan\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mastan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading google_api_python_client-2.168.0-py3-none-any.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/13.3 MB 1.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/13.3 MB 1.7 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.3/13.3 MB 1.5 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.6/13.3 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.8/13.3 MB 1.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.8/13.3 MB 1.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.1/13.3 MB 1.3 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/13.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.6/13.3 MB 1.2 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.9/13.3 MB 1.2 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 3.1/13.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.4/13.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.7/13.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.9/13.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.2/13.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 4.2/13.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.5/13.3 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.7/13.3 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 5.0/13.3 MB 1.2 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 5.2/13.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.5/13.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.8/13.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.8/13.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 6.0/13.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 6.3/13.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 6.6/13.3 MB 1.1 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 6.8/13.3 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 7.1/13.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.3/13.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.6/13.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.6/13.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 7.9/13.3 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 8.1/13.3 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 8.1/13.3 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 8.4/13.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 8.7/13.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 8.9/13.3 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 9.2/13.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.4/13.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 9.7/13.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 10.0/13.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 10.2/13.3 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.7/13.3 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 11.0/13.3 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.3/13.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.8/13.3 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.1/13.3 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.3/13.3 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.8/13.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 1.2 MB/s eta 0:00:00\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 1.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 1.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.8/4.3 MB 931.2 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.0/4.3 MB 949.8 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.0/4.3 MB 949.8 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 1.3/4.3 MB 817.9 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 1.3/4.3 MB 817.9 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.6/4.3 MB 814.1 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 860.4 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 2.1/4.3 MB 883.1 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 2.4/4.3 MB 900.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 2.4/4.3 MB 900.8 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 2.6/4.3 MB 883.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.9/4.3 MB 892.3 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.1/4.3 MB 891.4 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 3.4/4.3 MB 923.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.3 MB 956.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 1.0 MB/s eta 0:00:00\n",
      "Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Using cached protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, grpcio, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.168.0 google-auth-2.39.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.4 rsa-4.9.1 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b49f24ea-e5aa-4d82-b4ad-69654c90e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai import GenerativeModel, configure\n",
    " \n",
    "GEMINI_API_KEY = \"AIzaSyBCKHWtEYx3T_2v648uU22KkH-eVtZbvNU\"\n",
    "configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7bb7cef5-3002-4649-b711-2504278eda97",
   "metadata": {},
   "outputs": [],
   "source": [
    " def stream_gemini(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"parts\": [{\"text\": prompt}]}\n",
    "    ]\n",
    "\n",
    "    response = gemini_model.generate_content(messages,stream=True)\n",
    "    accumulated_text = \"\"\n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, 'text'):\n",
    "            accumulated_text += chunk.text   \n",
    "            yield accumulated_text  \n",
    "        else:\n",
    "            yield \"Error: Chunk does not contain text.\"\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0101f9f8-1537-40b1-9211-9fafc2ee42d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object stream_gemini at 0x000001A9DF716980>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_gemini(\"explain about llm engineering\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "879f57fa-768f-4095-94c5-1bec5e0a4315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7885\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "022d2a6d-79f9-44de-ae14-e1f4870ceb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt=\"You are an AI Assistant i need you to give all the response using markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ac1579c6-1fc1-4d10-a8a8-120c4a60a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aeb4ad50-5bf4-4af0-822f-0c7276f19c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\"),gr.Dropdown([\"GPT\", \"Gemini\"], label=\"Select model\", value=\"GPT\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "65472d51-b16e-4f9a-9418-86b661b25fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def stream_gemini(prompt):\n",
    "    messages = [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}]\n",
    "    \n",
    "    # Initialize an empty string to accumulate the output\n",
    "    accumulated_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Send request with streaming enabled\n",
    "        response = gemini_model.generate_content(messages, stream=True)\n",
    "        \n",
    "        # Ensure response is valid\n",
    "        if response is None:\n",
    "            print(\"Error: No response from the model.\")\n",
    "            return \"No response received from the model.\"\n",
    "        \n",
    "        # Yield the streamed response, accumulating text\n",
    "        for chunk in response:\n",
    "            if hasattr(chunk, 'text'):\n",
    "                accumulated_text += chunk.text  # Accumulate the text\n",
    "                yield accumulated_text  # Yield the accumulated text\n",
    "            else:\n",
    "                yield \"Error: Chunk does not contain text.\"\n",
    "                \n",
    "    except Exception as e:\n",
    "        yield f\"An error occurred: {e}\"\n",
    "\n",
    "# Set up the Gradio interface\n",
    "view = gr.Interface(\n",
    "    fn=stream_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "  \n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "view.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902d701-b48f-4a84-a22a-12cbec8eb9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
